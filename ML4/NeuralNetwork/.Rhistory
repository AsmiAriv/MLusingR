m = 5
lambda = lambda
# We generate some 'random' test data
Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)
Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)
# Reusing debugInitializeWeights to generate X
X  = debugInitializeWeights(m, input_layer_size - 1)
y  = 1 + t(1:m%%num_labels)
# Unroll parameters
nn_params = c(as.vector(Theta1), as.vector(Theta2))
# Short hand for cost function
costFunc = function(p){
cost = nnCostFunction(p, input_layer_size,
hidden_layer_size, num_labels,
X, y, lambda)
J = cost$J
grad = cost$grad
list(J=J, grad=grad)
}
res1 <- costFunc(nn_params)
grad <- res1$grad
J <- res1$J
numgrad = computeNumericalGradient(costFunc, nn_params)
df <- data.frame(grad=grad, numgrad=numgrad)
A <- as.matrix((numgrad-grad))
diff = norm(svd(A)$d)/norm(svd(A)$d)
list(df=df, diff=diff)
#df
}
test <- checkNNGradients(3)
checkNNGradients <- function(lambda){
#CHECKNNGRADIENTS Creates a small neural network to check the
#backpropagation gradients
#   CHECKNNGRADIENTS(lambda) Creates a small neural network to check the
#   backpropagation gradients, it will output the analytical gradients
#   produced by your backprop code and the numerical gradients (computed
#  using computeNumericalGradient). These two gradient computations should
#   result in very similar values.
input_layer_size = 3
hidden_layer_size = 5
num_labels = 3
m = 5
lambda = lambda
# We generate some 'random' test data
Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)
Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)
# Reusing debugInitializeWeights to generate X
X  = debugInitializeWeights(m, input_layer_size - 1)
y  = 1 + t(1:m%%num_labels)
# Unroll parameters
nn_params = c(as.vector(Theta1), as.vector(Theta2))
# Short hand for cost function
costFunc = function(p){
cost = nnCostFunction(p, input_layer_size,
hidden_layer_size, num_labels,
X, y, lambda)
J = cost$J
grad = cost$grad
list(J=J, grad=grad)
}
res1 <- costFunc(nn_params)
grad <- res1$grad
J <- res1$J
numgrad = computeNumericalGradient(costFunc, nn_params)
df <- data.frame(grad=grad, numgrad=numgrad)
A <- as.matrix((df[,2]-df[,1]))
diff = norm(svd(A)$d)/norm(svd(A)$d)
list(df=df, diff=diff)
#df
}
checkNNGradients(3)
checkNNGradients <- function(lambda){
#CHECKNNGRADIENTS Creates a small neural network to check the
#backpropagation gradients
#   CHECKNNGRADIENTS(lambda) Creates a small neural network to check the
#   backpropagation gradients, it will output the analytical gradients
#   produced by your backprop code and the numerical gradients (computed
#  using computeNumericalGradient). These two gradient computations should
#   result in very similar values.
input_layer_size = 3
hidden_layer_size = 5
num_labels = 3
m = 5
lambda = lambda
# We generate some 'random' test data
Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)
Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)
# Reusing debugInitializeWeights to generate X
X  = debugInitializeWeights(m, input_layer_size - 1)
y  = 1 + t(1:m%%num_labels)
# Unroll parameters
nn_params = c(as.vector(Theta1), as.vector(Theta2))
# Short hand for cost function
costFunc = function(p){
cost = nnCostFunction(p, input_layer_size,
hidden_layer_size, num_labels,
X, y, lambda)
J = cost$J
grad = cost$grad
list(J=J, grad=grad)
}
res1 <- costFunc(nn_params)
grad <- res1$grad
J <- res1$J
numgrad = computeNumericalGradient(costFunc, nn_params)
df <- data.frame(grad=grad, numgrad=numgrad)
A <- as.matrix(as.numeric(df[,2]-df[,1]))
diff = norm(svd(A)$d)/norm(svd(A)$d)
list(df=df, diff=diff)
#df
}
checkNNGradients(3)
checkNNGradients <- function(lambda){
#CHECKNNGRADIENTS Creates a small neural network to check the
#backpropagation gradients
#   CHECKNNGRADIENTS(lambda) Creates a small neural network to check the
#   backpropagation gradients, it will output the analytical gradients
#   produced by your backprop code and the numerical gradients (computed
#  using computeNumericalGradient). These two gradient computations should
#   result in very similar values.
input_layer_size = 3
hidden_layer_size = 5
num_labels = 3
m = 5
lambda = lambda
# We generate some 'random' test data
Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)
Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)
# Reusing debugInitializeWeights to generate X
X  = debugInitializeWeights(m, input_layer_size - 1)
y  = 1 + t(1:m%%num_labels)
# Unroll parameters
nn_params = c(as.vector(Theta1), as.vector(Theta2))
# Short hand for cost function
costFunc = function(p){
cost = nnCostFunction(p, input_layer_size,
hidden_layer_size, num_labels,
X, y, lambda)
J = cost$J
grad = cost$grad
list(J=J, grad=grad)
}
res1 <- costFunc(nn_params)
grad <- res1$grad
J <- res1$J
numgrad = computeNumericalGradient(costFunc, nn_params)
df <- data.frame(grad=grad, numgrad=numgrad)
#A <- as.matrix(as.numeric(df[,2]-df[,1]))
#diff = norm(svd(A)$d)/norm(svd(A)$d)
#list(df=df, diff=diff)
df
}
d <- checkNNGradients(3)
d
A <- as.matrix(d[,2]-d[,1])
A
s <- svd(A)
names(s)
B <- as.matrix(d[,2]+d[,1])
s2 <- svd(B)
diff <- norm(s$d)/norm(s2$d)
s$d
s$d/s2$d
diff <- norm(as.matrix(s$d))/norm(as.matrix(s2$d))
diff
checkNNGradients <- function(lambda){
#CHECKNNGRADIENTS Creates a small neural network to check the
#backpropagation gradients
#   CHECKNNGRADIENTS(lambda) Creates a small neural network to check the
#   backpropagation gradients, it will output the analytical gradients
#   produced by your backprop code and the numerical gradients (computed
#  using computeNumericalGradient). These two gradient computations should
#   result in very similar values.
input_layer_size = 3
hidden_layer_size = 5
num_labels = 3
m = 5
lambda = lambda
# We generate some 'random' test data
Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)
Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)
# Reusing debugInitializeWeights to generate X
X  = debugInitializeWeights(m, input_layer_size - 1)
y  = 1 + t(1:m%%num_labels)
# Unroll parameters
nn_params = c(as.vector(Theta1), as.vector(Theta2))
# Short hand for cost function
costFunc = function(p){
cost = nnCostFunction(p, input_layer_size,
hidden_layer_size, num_labels,
X, y, lambda)
J = cost$J
grad = cost$grad
list(J=J, grad=grad)
}
res1 <- costFunc(nn_params)
grad <- res1$grad
J <- res1$J
numgrad = computeNumericalGradient(costFunc, nn_params)
df <- data.frame(grad=grad, numgrad=numgrad)
#A <- as.matrix((df[,2]-df[,1]))
#B <- as.matrix((df[,2]+df[,1]))
diff = norm(as.matrix(svd(numgrad-grad)$d))/norm(as.matrix(svd(numgrad+grad)$d))
list(df=df, diff=diff)
#df
}
checkNNGradients(0)
checkNNGradients(3)
checkNNGradients(3)
checkNNGradients <- function(lambda){
#CHECKNNGRADIENTS Creates a small neural network to check the
#backpropagation gradients
#   CHECKNNGRADIENTS(lambda) Creates a small neural network to check the
#   backpropagation gradients, it will output the analytical gradients
#   produced by your backprop code and the numerical gradients (computed
#  using computeNumericalGradient). These two gradient computations should
#   result in very similar values.
input_layer_size = 3
hidden_layer_size = 5
num_labels = 3
m = 5
lambda = lambda
# We generate some 'random' test data
Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)
Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)
# Reusing debugInitializeWeights to generate X
X  = debugInitializeWeights(m, input_layer_size - 1)
y  = 1 + t(1:m%%num_labels)
# Unroll parameters
nn_params = c(as.vector(Theta1), as.vector(Theta2))
# Short hand for cost function
costFunc = function(p){
cost = nnCostFunction(p, input_layer_size,
hidden_layer_size, num_labels,
X, y, lambda)
J = cost$J
grad = cost$grad
list(J=J, grad=grad)
}
res1 <- costFunc(nn_params)
grad <- res1$grad
J <- res1$J
numgrad = computeNumericalGradient(costFunc, nn_params)
df <- data.frame(grad=grad, numgrad=numgrad)
cat(df)
cat('The above two columns you get should be very similar.\n',
'(Left-Your Numerical Gradient, Right-Analytical Gradient)\n\n')
#A <- as.matrix((df[,2]-df[,1]))
#B <- as.matrix((df[,2]+df[,1]))
diff = norm(as.matrix(svd(numgrad-grad)$d))/norm(as.matrix(svd(numgrad+grad)$d))
cat('If your backpropagation implementation is correct, then \n',
'the relative difference will be small (less than 1e-9). \n',
'\nRelative Difference:',diff)
#list(df=df, diff=diff)
#df
}
checkNNGradients(3)
checkNNGradients <- function(lambda){
#CHECKNNGRADIENTS Creates a small neural network to check the
#backpropagation gradients
#   CHECKNNGRADIENTS(lambda) Creates a small neural network to check the
#   backpropagation gradients, it will output the analytical gradients
#   produced by your backprop code and the numerical gradients (computed
#  using computeNumericalGradient). These two gradient computations should
#   result in very similar values.
input_layer_size = 3
hidden_layer_size = 5
num_labels = 3
m = 5
lambda = lambda
# We generate some 'random' test data
Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)
Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)
# Reusing debugInitializeWeights to generate X
X  = debugInitializeWeights(m, input_layer_size - 1)
y  = 1 + t(1:m%%num_labels)
# Unroll parameters
nn_params = c(as.vector(Theta1), as.vector(Theta2))
# Short hand for cost function
costFunc = function(p){
cost = nnCostFunction(p, input_layer_size,
hidden_layer_size, num_labels,
X, y, lambda)
J = cost$J
grad = cost$grad
list(J=J, grad=grad)
}
res1 <- costFunc(nn_params)
grad <- res1$grad
J <- res1$J
numgrad = computeNumericalGradient(costFunc, nn_params)
df <- data.frame(grad=grad, numgrad=numgrad)
return(df)
cat('The above two columns you get should be very similar.\n',
'(Left-Your Numerical Gradient, Right-Analytical Gradient)\n\n')
#A <- as.matrix((df[,2]-df[,1]))
#B <- as.matrix((df[,2]+df[,1]))
diff = norm(as.matrix(svd(numgrad-grad)$d))/norm(as.matrix(svd(numgrad+grad)$d))
cat('If your backpropagation implementation is correct, then \n',
'the relative difference will be small (less than 1e-9). \n',
'\nRelative Difference:',diff)
#list(df=df, diff=diff)
#df
}
checkNNGradients(3)
checkNNGradients <- function(lambda){
#CHECKNNGRADIENTS Creates a small neural network to check the
#backpropagation gradients
#   CHECKNNGRADIENTS(lambda) Creates a small neural network to check the
#   backpropagation gradients, it will output the analytical gradients
#   produced by your backprop code and the numerical gradients (computed
#  using computeNumericalGradient). These two gradient computations should
#   result in very similar values.
input_layer_size = 3
hidden_layer_size = 5
num_labels = 3
m = 5
lambda = lambda
# We generate some 'random' test data
Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)
Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)
# Reusing debugInitializeWeights to generate X
X  = debugInitializeWeights(m, input_layer_size - 1)
y  = 1 + t(1:m%%num_labels)
# Unroll parameters
nn_params = c(as.vector(Theta1), as.vector(Theta2))
# Short hand for cost function
costFunc = function(p){
cost = nnCostFunction(p, input_layer_size,
hidden_layer_size, num_labels,
X, y, lambda)
J = cost$J
grad = cost$grad
list(J=J, grad=grad)
}
res1 <- costFunc(nn_params)
grad <- res1$grad
J <- res1$J
numgrad = computeNumericalGradient(costFunc, nn_params)
df <- data.frame(grad=grad, numgrad=numgrad)
print(df)
cat('The above two columns you get should be very similar.\n',
'(Left-Your Numerical Gradient, Right-Analytical Gradient)\n\n')
#A <- as.matrix((df[,2]-df[,1]))
#B <- as.matrix((df[,2]+df[,1]))
diff = norm(as.matrix(svd(numgrad-grad)$d))/norm(as.matrix(svd(numgrad+grad)$d))
cat('If your backpropagation implementation is correct, then \n',
'the relative difference will be small (less than 1e-9). \n',
'\nRelative Difference:',diff)
#list(df=df, diff=diff)
#df
}
checkNNGradients(3)
checkNNGradients <- function(lambda){
#CHECKNNGRADIENTS Creates a small neural network to check the
#backpropagation gradients
#   CHECKNNGRADIENTS(lambda) Creates a small neural network to check the
#   backpropagation gradients, it will output the analytical gradients
#   produced by your backprop code and the numerical gradients (computed
#  using computeNumericalGradient). These two gradient computations should
#   result in very similar values.
input_layer_size = 3
hidden_layer_size = 5
num_labels = 3
m = 5
lambda = lambda
# We generate some 'random' test data
Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)
Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)
# Reusing debugInitializeWeights to generate X
X  = debugInitializeWeights(m, input_layer_size - 1)
y  = 1 + t(1:m%%num_labels)
# Unroll parameters
nn_params = c(as.vector(Theta1), as.vector(Theta2))
# Short hand for cost function
costFunc = function(p){
cost = nnCostFunction(p, input_layer_size,
hidden_layer_size, num_labels,
X, y, lambda)
J = cost$J
grad = cost$grad
list(J=J, grad=grad)
}
res1 <- costFunc(nn_params)
grad <- res1$grad
J <- res1$J
numgrad = computeNumericalGradient(costFunc, nn_params)
df <- data.frame(numgrad=numgrad,grad=grad)
print(df)
cat('The above two columns you get should be very similar.\n',
'(Left-Your Numerical Gradient, Right-Analytical Gradient)\n\n')
#A <- as.matrix((df[,2]-df[,1]))
#B <- as.matrix((df[,2]+df[,1]))
diff = norm(as.matrix(svd(numgrad-grad)$d))/norm(as.matrix(svd(numgrad+grad)$d))
cat('If your backpropagation implementation is correct, then \n',
'the relative difference will be small (less than 1e-9). \n',
'\nRelative Difference:',diff)
#list(df=df, diff=diff)
#df
}
checkNNGradients(3)
debug_J  = nnCostFunction(nn_params, input_layer_size,hidden_layer_size, num_labels, X, y, lambda);
debug_J <- debug_J$J
cat('\n\nCost at (fixed) debugging parameters (w/ lambda = 10):',
'\n(this value should be about 0.576051)\n\n', debug_J)
lambda = 10
debug_J  = nnCostFunction(nn_params, input_layer_size,hidden_layer_size, num_labels, X, y, lambda);
debug_J <- debug_J$J
cat('\n\nCost at (fixed) debugging parameters (w/ lambda = 10):',
'\n(this value should be about 0.576051)\n\n', debug_J)
checkNNGradients(3)
debug_J  = nnCostFunction(nn_params, input_layer_size,hidden_layer_size, num_labels, X, y, lambda);
debug_J <- debug_J$J
cat('\n\nCost at (fixed) debugging parameters (w/ lambda = 10):',
'\n(this value should be about 0.576051)\n\n', debug_J)
checkNNGradients(lambda=10)
lambda=10
debug_J  = nnCostFunction(nn_params, input_layer_size,hidden_layer_size, num_labels, X, y, lambda);
debug_J <- debug_J$J
cat('\n\nCost at (fixed) debugging parameters (w/ lambda = 10):',
'\n(this value should be about 0.576051)\n\n', debug_J)
input_layer_size = 3
hidden_layer_size = 5
num_labels = 3
m = 5
Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)
Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)
X  = debugInitializeWeights(m, input_layer_size - 1)
y  = 1 + t(1:m%%num_labels)
nn_params = c(as.vector(Theta1), as.vector(Theta2))
debug_J  = nnCostFunction(nn_params, input_layer_size,hidden_layer_size, num_labels, X, y, lambda);
debug_J <- debug_J$J
cat('\n\nCost at (fixed) debugging parameters (w/ lambda = 10):',
'\n(this value should be about 0.576051)\n\n', debug_J)
lambda
lambda <- 3
debug_J  = nnCostFunction(nn_params, input_layer_size,hidden_layer_size, num_labels, X, y, lambda);
debug_J <- debug_J$J
cat('\n\nCost at (fixed) debugging parameters (w/ lambda = 10):',
'\n(this value should be about 0.576051)\n\n', debug_J)
lambda <- 0
debug_J  = nnCostFunction(nn_params, input_layer_size,hidden_layer_size, num_labels, X, y, lambda);
debug_J <- debug_J$J
cat('\n\nCost at (fixed) debugging parameters (w/ lambda = 10):',
'\n(this value should be about 0.576051)\n\n', debug_J)
lambda <- 1
debug_J  = nnCostFunction(nn_params, input_layer_size,hidden_layer_size, num_labels, X, y, lambda);
debug_J <- debug_J$J
cat('\n\nCost at (fixed) debugging parameters (w/ lambda = 10):',
'\n(this value should be about 0.576051)\n\n', debug_J)
input_layer_size  = 400  # 20x20 Input Images of Digits
hidden_layer_size = 25   # 25 hidden units
num_labels = 10
X <- data[[1]]
y <- data[[2]]
Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)
Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)
nn_params = c(as.vector(Theta1), as.vector(Theta2))
lambda
debug_J  = nnCostFunction(nn_params, input_layer_size,hidden_layer_size, num_labels, X, y, lambda);
debug_J <- debug_J$J
cat('\n\nCost at (fixed) debugging parameters (w/ lambda = 10):',
'\n(this value should be about 0.576051)\n\n', debug_J)
X  = debugInitializeWeights(m, input_layer_size - 1)
y  = 1 + t(1:m%%num_labels)
debug_J  = nnCostFunction(nn_params, input_layer_size,hidden_layer_size, num_labels, X, y, lambda);
debug_J <- debug_J$J
cat('\n\nCost at (fixed) debugging parameters (w/ lambda = 10):',
'\n(this value should be about 0.576051)\n\n', debug_J)
#Loading sigmoid function
source('sigmoid.R')
#Loading sigmoidGradient function
source('sigmoidGradient.R')
#Loading neural network cost function
source('nnCostFunction.R')
#Loading checkNNGradients function
source('checkNNGradients.R')
#Loading computeNumericalGradient function
source('computeNumericalGradient.R')
#Loading debugInitializeWeights function
source('debugInitializeWeights.R')
#Loading randInitializeWeights function
source('randInitializeWeights.R')
#Loading predict function
source('predict.R')
#Loading data display function
source('displayData.R')
checkNNGradients(3)
debug_J  = nnCostFunction(nn_params, input_layer_size,hidden_layer_size, num_labels, X, y, lambda);
J <- debug_J$J
J
rm(list=ls())
#Loading sigmoid function
source('sigmoid.R')
#Loading sigmoidGradient function
source('sigmoidGradient.R')
#Loading neural network cost function
source('nnCostFunction.R')
#Loading checkNNGradients function
source('checkNNGradients.R')
#Loading computeNumericalGradient function
source('computeNumericalGradient.R')
#Loading debugInitializeWeights function
source('debugInitializeWeights.R')
#Loading randInitializeWeights function
source('randInitializeWeights.R')
#Loading predict function
source('predict.R')
#Loading data display function
source('displayData.R')
checkNNGradients(3)
debug_J  = nnCostFunction(nn_params, input_layer_size,hidden_layer_size, num_labels, X, y, lambda);
