theta
cost[2000]
num_iter = 300
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
cost[300]
theta
hes
num_iter = 200
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
hes
theta
cost[200]
num_iter = 100
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
theta
num_iter = 150
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
hes
theta
cost[150]
grad
num_iter = 150
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
hes
cost[150]
cost[140;150]
cost[140:150]
cost[1:10]
num_iter = 200
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
cost[190:200]
cost[150:200]
dim(grad)
dim(hes)
dim(X)
source('gradThetaNewtonReg.R')
num_iter = 400
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
cost[400]
hes
theta
grad
cost[350:400]
dim(hes)
source('gradThetaNewtonReg.R')
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
?diag
dim(X)
source('gradThetaNewtonReg.R')
source('gradThetaNewtonReg.R')
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
cost[400]
hes
theta
dim(hes)
source('gradThetaNewtonReg.R')
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
cost[400]
theta
num_iter = 2000
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
cost[2000]
cost[1950:2000]
cost[350:400]
cost[50:100]
cost[110:100]
num_iter = 100
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
theta
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
theta
cost[100]
num_iter = 400
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
cost[400]
theta
p = predict(theta, X)
cat('Train Accuracy:\n',mean((p == y))*100)
plot(1:num_iter,cost)
num_iter = 3000
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
p = predict(theta, X)
cat('Train Accuracy:\n',mean((p == y))*100)
plot(1:num_iter,cost)
source('gradThetaNewtonReg.R')
num_iter = 400
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
plot(1:num_iter,cost)
p = predict(theta, X)
cat('Train Accuracy:\n',mean((p == y))*100)
theta
num_iter = 3000
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
plot(1:num_iter,cost)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
p = predict(theta, X)
cat('Train Accuracy:\n',mean((p == y))*100)
plot(1:num_iter,cost)
theta
source('gradThetaNewtonReg.R')
num_iter = 400
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
num_iter = 400
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
cost[400]
theta
p = predict(theta, X)
cat('Train Accuracy:\n',mean((p == y))*100)
plot(1:num_iter,cost)
source('gradThetaNewtonReg.R')
num_iter = 400
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
p = predict(theta, X)
cat('Train Accuracy:\n',mean((p == y))*100)
plot(1:num_iter,cost)
?ginv
dim(des)
dim(hes)
dim(ginv(hes)
)
source('gradThetaNewtonReg.R')
num_iter = 400
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
plot(1:num_iter,cost)
theta
source('gradThetaNewtonReg.R')
num_iter = 400
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
plot(1:num_iter,cost)
source('gradThetaNewtonReg.R')
num_iter = 400
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
plot(1:num_iter,cost)
source('gradThetaNewtonReg.R')
num_iter = 400
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
plot(1:num_iter,cost)
theta
cost[390:400]
data <- read.table('ex2data1.txt',sep=",")
X <- data[,1:2]
y <- data[,3]
m = length(y)
#Loading cost function
source('costFunction.R')
#Loading gradThetaNewton function
source('gradThetaNewton.R')
#Loading cost function with regularization
source('costFunctionReg.R')
#Loading predict function
source('predict.R')
#Loading sigmoid function
source('sigmoid.R')
#Loading mapFeature function
source('mapFeature.R')
#Loading plotData function
source('plotData.R')
#Loading plotDecisionBoundary function
source('plotDecisionBoundary.R')
plotData(X, y)
X <- cbind(rep(1,m),X)
X <- as.matrix(X)
#Initializing the fitting parameters
initial_theta <- rep(0,dim(X)[2])
num_iter = 400
costh <- gradThetaNewton (initial_theta, X,y,num_iter)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history[num_iter]
cat('Cost at theta found after optimization:', '\n', cost)
cat('theta found after optimization:', '\n',theta)
cat('Gradient found after optimization:', '\n',grad)
prob = sigmoid(c(1, 45, 85)%*%theta)
cat('For a student with scores 45 and 85, we predict an admission probability of \n',prob)
p = predict(theta, X)
cat('Train Accuracy:\n',mean((p == y))*100)
plot(1:num_iter,cost)
plot(1:num_iter,costh$J_history)
source('gradThetaNewton.R')
costh <- gradThetaNewton (initial_theta, X,y,num_iter)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history[num_iter]
plot(1:num_iter,costh$J_history)
cat('Cost at theta found after optimization:', '\n', cost)
cat('theta found after optimization:', '\n',theta)
cat('Gradient found after optimization:', '\n',grad)
prob = sigmoid(c(1, 45, 85)%*%theta)
cat('For a student with scores 45 and 85, we predict an admission probability of \n',prob)
p = predict(theta, X)
cat('Train Accuracy:\n',mean((p == y))*100)
source('gradThetaNewtonReg.R')
num_iter = 100
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history
hes <- costh$h
cost[100]
cat('Cost at theta found after optimization:', '\n', cost[num_iter])
cat('theta found after optimization:', '\n',theta)
cat('Gradient found after optimization:', '\n',grad)
data <- read.table('ex2data2.txt',sep=",")
X <- data[,1:2]
y <- data[,3]
m = length(y)
X = mapFeature(X[,1], X[,2])
initial_theta <- rep(0,dim(X)[2])
lambda = 1
num_iter = 100
costh <- gradThetaNewtonReg(initial_theta, X, y,num_iter,lambda)
data <- read.table('ex2data2.txt',sep=",")
X <- data[,1:2]
y <- data[,3]
m = length(y)
plotData(X, y)
head(y)
unique(y)
head(X)
plot(X[,1],X[,2])
pos <- y[y==1]
neg <- y[y==0]
plot(X[pos,1],X[neg,2])
plot(X[pos,1],X[pos,2],col="yellow")
plot(X[pos,1],X[pos,2],col="red")
length(pos)
length(neg)
pos
pos <- which()[y==1]
pos <- which([y==1])
pos <- which(y==1)
pos
plotData(X, y)
source('plotData.R')
plotData(X, y)
source('plotDecisionBoundary.R')
X = mapFeature(X[,1], X[,2])
#Initializing the fitting parameters
initial_theta <- rep(0,dim(X)[2])
# Set regularization parameter lambda to 1 (you should vary this)
lambda = 1
#Compute and display theta and cost using optim() as optimizer
#num_iter = 400
costh <- optim(par=initial_theta, fn=costFunctionReg, gr=gradReg, method="BFGS", X=X,y=y,lambda=lambda)
theta <- costh$par
#grad <- costh$gradient
cost <- costh$value
cat('Cost at theta found after optimization:', '\n', cost)
cat('theta found after optimization:', '\n',theta)
#cat('Gradient found after optimization:', '\n',grad)
plotDecisionBoundary(theta, X, y)
u = seq(-1, 1.5, length.out=50)
v = seq(-1, 1.5, length.out=50)
u
z = matrix(rep(0,length(u)*length(v)),length(u))
z
plotDecisionBoundary(theta, X, y)
source('plotDecisionBoundary.R')
plotDecisionBoundary(theta, X, y)
?contourLines
source('plotDecisionBoundary.R')
plotDecisionBoundary(theta, X, y)
?contour
source('plotDecisionBoundary.R')
plotDecisionBoundary(theta, X, y)
source('plotDecisionBoundary.R')
plotDecisionBoundary(theta, X, y)
?legend
source('plotDecisionBoundary.R')
plotDecisionBoundary(theta, X, y)
source('plotDecisionBoundary.R')
plotDecisionBoundary(theta, X, y)
source('plotDecisionBoundary.R')
plotDecisionBoundary(theta, X, y)
?pch
source('plotDecisionBoundary.R')
?legend
source('plotDecisionBoundary.R')
plotDecisionBoundary(theta, X, y)
?contour
source('plotDecisionBoundary.R')
plotDecisionBoundary(theta, X, y)
?pch
source('plotData.R')
#Loading plotDecisionBoundary function
source('plotDecisionBoundary.R')
plotDecisionBoundary(theta, X, y)
source('plotData.R')
#Loading plotDecisionBoundary function
source('plotDecisionBoundary.R')
plotDecisionBoundary(theta, X, y)
?pch
source('plotDecisionBoundary.R')
plotDecisionBoundary(theta, X, y)
sum(y==1)
sum(y==0)
#This file uses the following functions
# To run this script please ensure to store these functions in
# the same folder as this file and set this folder as working directory
#     sigmoid.R
#     costFunction.R
#     gradThetaNewton.R
#     predict.R
#     costFunctionReg.R
#     plotData.R
#     mapFeature.R
#     plotDecisionBoundary.R
#Setting the working directory
setwd("C:/R/MachineLearning/ML2/LogisticRegression")
# loading data
data <- read.table('ex2data1.txt',sep=",")
X <- data[,1:2]
y <- data[,3]
m = length(y)
#Loading cost function
source('costFunction.R')
#Loading gradThetaNewton function
source('gradThetaNewton.R')
#Loading cost function with regularization
source('costFunctionReg.R')
#Loading predict function
source('predict.R')
#Loading sigmoid function
source('sigmoid.R')
#Loading mapFeature function
source('mapFeature.R')
#Loading plotData function
source('plotData.R')
#Loading plotDecisionBoundary function
source('plotDecisionBoundary.R')
#Plotting data
plotData(X, y)
#Adding a column for intercept to the predictors X
X <- cbind(rep(1,m),X)
X <- as.matrix(X)
#Initializing the fitting parameters
initial_theta <- rep(0,dim(X)[2])
#Compute and display theta, cost and gradient after optimizatio
#using gradThetaNewton function
num_iter = 400
costh <- gradThetaNewton (initial_theta, X,y,num_iter)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history[num_iter]
cat('Cost at theta found after optimization:', '\n', cost)
cat('theta found after optimization:', '\n',theta)
cat('Gradient found after optimization:', '\n',grad)
#Plot Boundary
plotDecisionBoundary(theta, X, y)
#Predict probability for a student with score 45 on exam 1
#and score 85 on exam 2
prob = sigmoid(c(1, 45, 85)%*%theta)
cat('For a student with scores 45 and 85, we predict an admission probability of \n',prob)
p = predict(theta, X)
cat('Train Accuracy:\n',mean((p == y))*100)
?lines
source('plotDecisionBoundary.R')
plotDecisionBoundary(theta, X, y)
#This file uses the following functions
# To run this script please ensure to store these functions in
# the same folder as this file and set this folder as working directory
#     sigmoid.R
#     costFunction.R
#     gradThetaNewton.R
#     predict.R
#     costFunctionReg.R
#     plotData.R
#     mapFeature.R
#     plotDecisionBoundary.R
#Setting the working directory
setwd("C:/R/MachineLearning/ML2/LogisticRegression")
# loading data
data <- read.table('ex2data1.txt',sep=",")
X <- data[,1:2]
y <- data[,3]
m = length(y)
#Loading cost function
source('costFunction.R')
#Loading gradThetaNewton function
source('gradThetaNewton.R')
#Loading cost function with regularization
source('costFunctionReg.R')
#Loading predict function
source('predict.R')
#Loading sigmoid function
source('sigmoid.R')
#Loading mapFeature function
source('mapFeature.R')
#Loading plotData function
source('plotData.R')
#Loading plotDecisionBoundary function
source('plotDecisionBoundary.R')
#Plotting data
plotData(X, y)
#Adding a column for intercept to the predictors X
X <- cbind(rep(1,m),X)
X <- as.matrix(X)
#Initializing the fitting parameters
initial_theta <- rep(0,dim(X)[2])
#Compute and display theta, cost and gradient after optimizatio
#using gradThetaNewton function
num_iter = 400
costh <- gradThetaNewton (initial_theta, X,y,num_iter)
theta <- costh$theta
grad <- costh$gradient
cost <- costh$J_history[num_iter]
cat('Cost at theta found after optimization:', '\n', cost)
cat('theta found after optimization:', '\n',theta)
cat('Gradient found after optimization:', '\n',grad)
#Plot Boundary
plotDecisionBoundary(theta, X, y)
#Predict probability for a student with score 45 on exam 1
#and score 85 on exam 2
prob = sigmoid(c(1, 45, 85)%*%theta)
cat('For a student with scores 45 and 85, we predict an admission probability of \n',prob)
p = predict(theta, X)
cat('Train Accuracy:\n',mean((p == y))*100)
source('plotDecisionBoundary.R')
plotDecisionBoundary(theta, X, y)
plotDecisionBoundary(theta, X, y)
?pch
